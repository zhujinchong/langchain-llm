import nltk
import os
import shutil
import gradio as gr

from service.config import *
from service.langchain_application import LangChainApplication

nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path

# çŸ¥è¯†åº“åç§°å’Œè·¯å¾„
knowledge_map = {
    "æœ¬åœ°çŸ¥è¯†åº“": DATA_VECTOR_STORE_PATH + "init_local"
}
# å¯åŠ¨
application = LangChainApplication()
# åŠ è½½çŸ¥è¯†åº“
application.source_service.load_vector_store(knowledge_map["æœ¬åœ°çŸ¥è¯†åº“"])


# ä¸Šä¼ æ–‡ä»¶
def upload_file(file):
    if not os.path.exists("docs"):
        os.mkdir("docs")
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    application.source_service.add_document("docs/" + filename)


# æœ¬åœ°çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨
def local_file_list():
    file_list = []
    for doc in os.listdir(DATA_UPLOAD_PATH):
        filepath = DATA_UPLOAD_PATH + doc
        if filepath.endswith(".doc") or filepath.endswith(".docx"):
            file_list.append(filepath)
    return file_list
v_local_file_list = local_file_list()


# åˆ‡æ¢/åŠ è½½ çŸ¥è¯†åº“
def set_knowledge(kg_name, history):
    try:
        application.source_service.load_vector_store(knowledge_map[kg_name])
        msg_status = f'{kg_name}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        print(e)
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]


def clear_session():
    return '', [], []


def predict(input,
            top_k,
            doc_score,
            use_pattern,
            chatbot,
            history,
            max_token,
            top_p,
            temperature):
    if history == None:
        history = []
    if chatbot == None:
        chatbot = []
    # if use_web == 'ä½¿ç”¨':
    #     web_content = application.source_service.search_web(query=input)
    # else:
    #     web_content = ''
    chatbot.append((input, ""))
    if use_pattern == 'æ¨¡å‹é—®ç­”':
        for resp, history in application.get_llm_answer(input, history, "", max_token, temperature, top_p):
            chatbot[-1] = (input, resp)
            yield '', chatbot, history
    else:
        if application.source_service.vector_store:
            for resp, history in application.get_knowledge_based_answer(input, history, "", max_token, temperature, top_p, doc_score, top_k):
                chatbot[-1] = (input, resp["result"])
                yield '', chatbot, history
            source = resp["result"] + "\n\n"
            source += "".join(
                [
                    f"""<details> <summary>å‡ºå¤„ [{i + 1}] {os.path.split(doc.metadata["source"])[-1]} ç›¸ä¼¼åº¦ {doc.metadata["score"]}</summary>\n"""
                    f"""{doc.page_content}\n"""
                    f"""</details>"""
                    for i, doc in
                    enumerate(resp["source_documents"])])
            chatbot[-1] = (input, source)
            yield '', chatbot, history
        else:
            chatbot[-1] = (input, "è¯·å…ˆåŠ è½½çŸ¥è¯†åº“ï¼ï¼")
            yield '', chatbot, history


# with open("assets/custom.css", "r", encoding="utf-8") as f:
#     customCSS = f.read()
with gr.Blocks() as demo:
    gr.Markdown("""<h1><center>LangChain-ChatGLM2-6B</center></h1>
        <center><font size=3>
        </center></font>
        """)
    history = gr.State([])

    with gr.Row():
        # å·¦æ ï¼šé…ç½®
        with gr.Column(scale=1):
            use_pattern = gr.Radio(
                ['æ¨¡å‹é—®ç­”', 'çŸ¥è¯†åº“é—®ç­”'],
                label="æ¨¡å¼",
                value='æ¨¡å‹é—®ç­”',
                interactive=True)

            top_k = gr.Slider(1, 5, value=1, step=1, label="æ£€ç´¢top-kæ–‡æ¡£", interactive=True)
            doc_score = gr.Slider(300, 1000, value=800, step=1, label="æ–‡æ¡£åŒ¹é…åˆ†æ•°", interactive=True)

            kg_name = gr.Dropdown(
                list(knowledge_map.keys()),
                label="çŸ¥è¯†åº“",
                info="ä½¿ç”¨çŸ¥è¯†åº“é—®ç­”ï¼Œè¯·åŠ è½½çŸ¥è¯†åº“",
                value="æœ¬åœ°çŸ¥è¯†åº“",
                interactive=False)

            gr.Dropdown(
                list(v_local_file_list),
                label="å·²åŠ è½½æ–‡æ¡£",
                info="åŠ è½½æ›´å¤šæ–‡æ¡£ï¼Œè¯·è”ç³»ç®¡ç†å‘˜",
                value=v_local_file_list[0],
                interactive=True)

            file = gr.File(
                label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…",
                visible=True,
                file_types=['.txt', '.md', '.docx', '.pdf'],
                interactive=False)

        # ä¸­é—´æ ï¼šå¯¹è¯
        with gr.Column(scale=4):
            chatbot = gr.Chatbot().style(height=600)
            with gr.Row():
                with gr.Column(scale=4):
                    message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜', lines=4)
                    with gr.Row():
                        clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                        send = gr.Button("ğŸš€ å‘é€")
                with gr.Column(scale=1):
                    max_token = gr.Slider(100, 10000, value=8192, step=1, label="Maximum length", interactive=True)
                    top_p = gr.Slider(0.1, 1, value=0.8, step=0.01, label="Top p", interactive=True)
                    temperature = gr.Slider(0.1, 1, value=0.95, step=0.01, label="Temperature", interactive=True)

        # ============= è§¦å‘åŠ¨ä½œ=============
        # ä¸Šä¼ æ–‡ä»¶
        file.upload(upload_file,
                    inputs=file,
                    outputs=None)

        # åˆ‡æ¢çŸ¥è¯†åº“
        kg_name.change(
            set_knowledge,
            show_progress=True,
            inputs=[kg_name, chatbot],
            outputs=chatbot)

        # æäº¤è¾“å…¥
        send.click(
            predict,
            inputs=[
                message,
                top_k,
                doc_score,
                use_pattern,
                chatbot,
                history,
                max_token,
                top_p,
                temperature
            ],
            outputs=[message, chatbot, history])

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(
            fn=clear_session,
            inputs=[],
            outputs=[message, chatbot, history],
            queue=False)

demo.queue(concurrency_count=10).launch(
    server_name='0.0.0.0',
    server_port=7888,
    share=False,
)
